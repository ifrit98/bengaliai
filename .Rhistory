s = example.SerializeToString()
writer.write(s)
def _build_json_dict(batch):
assert isinstance(batch, dict)
batch_keys = batch.keys()
tensor_metadata = ['fnc', 'dtype', 'shape', 'previous_dtype']
features = dict.fromkeys(batch_keys)
for field in batch_keys:
features[field] = dict.fromkeys(tensor_metadata)
# excluding batch dim
if len(batch[field].shape[1:]) == 1 and batch[field].shape[1] == 1:
features[field]['fnc'] = 'FixedLen'
else:
features[field]['fnc'] = 'FixedLenSequence'
# get shape
features[field]['shape'] = batch[field].shape[1:]  # exclude batch dim
# get previous dtype
features[field]['previous_dtype'] = batch[field].dtype.name
if 'complex' in features[field]['previous_dtype']:
# complex128 will get casted to complex64 later
features[field]['previous_dtype'] = 'complex64'
# get fnc to use while reading
fnc = FEAT_FNC_SWITCH[batch[field].dtype.name]
# get dtype
if 'float' in fnc.__name__:
dtype = 'tf.float32'
elif 'complex' in fnc.__name__:
dtype = 'tf.float32'
elif 'int' in fnc.__name__:
dtype = 'tf.int64'
elif 'bytes' in fnc.__name__:
dtype = 'tf.string'
else:
raise ValueError('Cant find datatype')
features[field]['dtype'] = dtype
return features
def batch_to_nparray(batch):
np_batch = { k: np.asarray(v) for k,v in batch.items() }
return np_batch
def _build_json_dict(batch):
assert isinstance(batch, dict)
batch_keys = batch.keys()
tensor_metadata = ['fnc', 'dtype', 'shape', 'previous_dtype']
features = dict.fromkeys(batch_keys)
for field in batch_keys:
features[field] = dict.fromkeys(tensor_metadata)
if len(batch[field].shape[1:]) == 1 and batch[field].shape[1] == 1:
features[field]['fnc'] = 'FixedLen'
else:
features[field]['fnc'] = 'FixedLenSequence'
# get shape
features[field]['shape'] = batch[field].shape[1:]  # exclude batch dim
# get previous dtype
features[field]['previous_dtype'] = batch[field].dtype.name
if 'complex' in features[field]['previous_dtype']:
# complex128 will get casted to complex64 later
features[field]['previous_dtype'] = 'complex64'
# get fnc to use while reading
fnc = FEAT_FNC_SWITCH[batch[field].dtype.name]
# get dtype
if 'float' in fnc.__name__:
dtype = 'tf.float32'
elif 'complex' in fnc.__name__:
dtype = 'tf.float32'
elif 'int' in fnc.__name__:
dtype = 'tf.int64'
elif 'bytes' in fnc.__name__:
dtype = 'tf.string'
else:
raise ValueError('Cant find datatype')
features[field]['dtype'] = dtype
return features
def _build_json_dict(batch):
assert isinstance(batch, dict)
batch_keys = batch.keys()
tensor_metadata = ['fnc', 'dtype', 'shape', 'previous_dtype']
features = dict.fromkeys(batch_keys)
for field in batch_keys:
features[field] = dict.fromkeys(tensor_metadata)
if len(batch[field].shape[1:]) == 1 and batch[field].shape[1] == 1:
features[field]['fnc'] = 'FixedLen'
else:
features[field]['fnc'] = 'FixedLenSequence'
# get shape
features[field]['shape'] = batch[field].shape[1:]  # exclude batch dim
# get previous dtype
features[field]['previous_dtype'] = batch[field].dtype.name
if 'complex' in features[field]['previous_dtype']:
# complex128 will get casted to complex64 later
features[field]['previous_dtype'] = 'complex64'
# get fnc to use while reading
fnc = FEAT_FNC_SWITCH[batch[field].dtype.name]
# get dtype
if 'float' in fnc.__name__:
dtype = 'tf.float32'
elif 'complex' in fnc.__name__:
dtype = 'tf.float32'
elif 'int' in fnc.__name__:
dtype = 'tf.int64'
elif 'bytes' in fnc.__name__:
dtype = 'tf.string'
else:
raise ValueError('Cant find datatype')
features[field]['dtype'] = dtype
return features
def _build_json_dict(batch):
assert isinstance(batch, dict)
batch_keys = batch.keys()
tensor_metadata = ['fnc', 'dtype', 'shape', 'previous_dtype']
features = dict.fromkeys(batch_keys)
for field in batch_keys:
features[field] = dict.fromkeys(tensor_metadata)
if len(batch[field].shape[1:]) == 1 and batch[field].shape[1] == 1:
features[field]['fnc'] = 'FixedLen'
else:
features[field]['fnc'] = 'FixedLenSequence'
# get shape
features[field]['shape'] = batch[field].shape[1:]  # exclude batch dim
# get previous dtype
features[field]['previous_dtype'] = batch[field].dtype.name
if 'complex' in features[field]['previous_dtype']:
# complex128 will get casted to complex64 later
features[field]['previous_dtype'] = 'complex64'
# get fnc to use while reading
fnc = FEAT_FNC_SWITCH[batch[field].dtype.name]
# get dtype
if 'float' in fnc.__name__:
dtype = 'tf.float32'
elif 'complex' in fnc.__name__:
dtype = 'tf.float32'
elif 'int' in fnc.__name__:
dtype = 'tf.int64'
elif 'bytes' in fnc.__name__:
dtype = 'tf.string'
else:
raise ValueError('Cant find datatype')
features[field]['dtype'] = dtype
return features
def _build_json_dict(batch):
assert isinstance(batch, dict)
batch_keys = batch.keys()
tensor_metadata = ['fnc', 'dtype', 'shape', 'previous_dtype']
features = dict.fromkeys(batch_keys)
for field in batch_keys:
features[field] = dict.fromkeys(tensor_metadata)
if len(batch[field].shape[1:]) == 1 and batch[field].shape[1] == 1:
features[field]['fnc'] = 'FixedLen'
else:
features[field]['fnc'] = 'FixedLenSequence'
# get shape
features[field]['shape'] = batch[field].shape[1:]  # exclude batch dim
# get previous dtype
features[field]['previous_dtype'] = batch[field].dtype.name
if 'complex' in features[field]['previous_dtype']:
# complex128 will get casted to complex64 later
features[field]['previous_dtype'] = 'complex64'
# get fnc to use while reading
fnc = FEAT_FNC_SWITCH[batch[field].dtype.name]
# get dtype
if 'float' in fnc.__name__:
dtype = 'tf.float32'
elif 'complex' in fnc.__name__:
dtype = 'tf.float32'
elif 'int' in fnc.__name__:
dtype = 'tf.int64'
elif 'bytes' in fnc.__name__:
dtype = 'tf.string'
else:
raise ValueError('Cant find datatype')
features[field]['dtype'] = dtype
return features
def batch_to_nparray(batch):
np_batch = { k: np.asarray(v) for k,v in batch.items() }
return np_batch
def _build_features(feature_dict, keys):
features = {}
for i in range(len(feature_dict)):
dtype = feature_dict[keys[i]].dtype.name
features[keys[i]] = FEAT_FNC_SWITCH[dtype](
feature_dict[keys[i]].ravel())
return features
def _write_to_tfrecord(batch, writer, batch_size=1):
# TODO: double check if this works with unbatched?  e.g. would send 32331 instead of 8
NUM_SAMPS = batch_size # next(iter(batch.value()).shape[0]
keys = list(batch.keys())
for i in range(NUM_SAMPS):
observation = {k: (v if batch_size == 1 else v[i]) for k, v in batch.items()}
features = _build_features(observation, keys)
example = tf.train.Example(
features=tf.train.Features(feature=features))
s = example.SerializeToString()
writer.write(s)
# TODO: work on getting replay generator up and running
def replay_generator(src_dir, parallel_files=1):
"""
Parameters
src_dir: Directory containing tfrecords to be read into memory
parallel_files: Number of files to read concurrently
Reads in tensorflow records and parses examples
Usage:
>>> dataset = replay_generator('/my/tfrecords/path', 1)
>>> sess = tf.Session()
>>> it = dataset.make_one_shot_iterator()
>>> nb = it.get_next()
>>> type(sess.run(nb))
<type 'dict'>
"""
# find json file in directory
src_dir =os.path.abspath(src_dir)
json_path = os.path.join(src_dir, JSON_FNAME)
assert os.path.isfile(json_path)
# bild feature dictionary from json
READ_FNCS = {
'FixedLen': tf.io.FixedLenFeature,
'FixedLenSequence': tf.io.FixedLenSequenceFeature
}
STR_TO_DTYPE = {
'tf.float32': tf.float32,
'tf.int32': tf.int32,
'tf.int64': tf.int64,
'tf.string': tf.string
}
with open(json_path, "r") as json_file:
feature_dicts = json.load(json_file)
features = {}
for field_key in feature_dicts.keys():
# remove odd 'u' from keys as a result of json
field_key = str(field_key)
for sub_key in feature_dicts[field_key].keys():
sub_key = str(sub_key)
fnc_str = feature_dicts[field_key]['fnc']
dtype_str = feature_dicts[field_key]['dtype']
shape = feature_dicts[field_key]['shape']
fnc = READ_FNCS[fnc_str]
dtype = STR_TO_DTYPE[dtype_str]
if fnc.__name__ == 'FixedLenSequenceFeature':
features[field_key] = fnc(shape, dtype, allow_missing=True)
else:
features[field_key] = fnc(shape, dtype)
dataset = tf.data.Dataset.list_files(src_dir + "/*.tfrecord")
dataset = dataset.interleave(tf.data.TFRecordDataset,
cycle_length=parallel_files)
# Parse the input `tf.Example` proto using the features dictionary.
def _parse_example(example_proto):
return tf.io.parse_single_example(example_proto, features)
return dataset.map(_parse_example)
def _assert_batch_has_expected_shape(batch, expected_shape, unbatch):
assert isinstance(batch, dict)
b_keys = batch.keys()
e_keys = expected_shape.keys()
if set(b_keys) != set(e_keys):
raise ValueError("Batch keys do not match json dictionary")
for key in b_keys:
if type(batch[key]) is not np.ndarray and type(batch[key]) is not np.array:
raise TypeError(
"Batch is a singular item, not np array ... maybe you meant to set unbatch=False?")
if batch[key].dtype.name != expected_shape[key]['previous_dtype']:
raise ValueError(
"Batch feature datatypes do not match json dictionary")
# exclude batch dim
if list(batch[key].shape[1:]) != list(expected_shape[key]['shape']):
raise ValueError(
"Batch feature shapes do not match json dictionary")
return True
def _assert_item_has_expected_shape(item, expected_shape, unbatch):
assert isinstance(item, dict)
r_keys = item.keys()
e_keys = expected_shape.keys()
if set(r_keys) != set(e_keys):
raise ValueError("Item key does not batch json dictionary")
for key in r_keys:
if item[key].dtype.name != expected_shape[key]['previous_dtype']:
raise ValueError(
"Item feature datatype does not batch json dictionary")
item_shape = item[key].shape if unbatch else item[key].shape[1:]
if list(item_shape) != list(expected_shape[key]['shape']):
print("key:", key)
print("item_shape:", list(item_shape))
print("expected_shape:", expected_shape[key]['shape'])
raise ValueError(
"Item shape does not match json dictionary")
return True
def _bytes_feature(value):
"""Returns a bytes_list from a string / byte."""
return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))
def _float_feature(value):
"""Returns a float_list from a float / double."""
return tf.train.Feature(float_list=tf.train.FloatList(value=value))\
def _complex_feature(value):
"""Returns a float list of length len(value)*2 from a complex array"""
value = value.astype(np.complex64)
temp = value.view(np.float32)
return tf.train.Feature(float_list=tf.train.FloatList(value=temp))
def _int64_feature(value):
"""Returns an int64_list from a bool / enum / int / uint."""
return tf.train.Feature(int64_list=tf.train.Int64List(value=value))
FEAT_FNC_SWITCH = {
"string":  _bytes_feature,
"byte":    _bytes_feature,
"object":  _bytes_feature,
"float32": _float_feature,
"float64": _float_feature,
"complex64":  _complex_feature,
"complex128": _complex_feature,
"bool":   _int64_feature,
"enum":   _int64_feature,
"int16":  _int64_feature,
"int32":  _int64_feature,
"uint32": _int64_feature,
"int64":  _int64_feature,
"uint64": _int64_feature
}
def _build_features(feature_dict, keys):
features = {}
for i in range(len(feature_dict)):
dtype = feature_dict[keys[i]].dtype.name
features[keys[i]] = FEAT_FNC_SWITCH[dtype](
feature_dict[keys[i]].ravel())
return features
def _write_to_tfrecord(batch, writer, batch_size=1):
NUM_SAMPS = batch_size # next(iter(batch.value()).shape[0]
keys = list(batch.keys())
for i in range(NUM_SAMPS):
observation = {k: (v if batch_size == 1 else v[i]) for k, v in batch.items()}
features = _build_features(observation, keys)
example = tf.train.Example(
features=tf.train.Features(feature=features))
s = example.SerializeToString()
writer.write(s)
def _build_json_dict(batch):
assert isinstance(batch, dict)
batch_keys = batch.keys()
tensor_metadata = ['fnc', 'dtype', 'shape', 'previous_dtype']
features = dict.fromkeys(batch_keys)
for field in batch_keys:
features[field] = dict.fromkeys(tensor_metadata)
if len(batch[field].shape[1:]) == 1 and batch[field].shape[1] == 1:
features[field]['fnc'] = 'FixedLen'
else:
features[field]['fnc'] = 'FixedLenSequence'
# get shape
features[field]['shape'] = batch[field].shape[1:]  # exclude batch dim
# get previous dtype
features[field]['previous_dtype'] = batch[field].dtype.name
if 'complex' in features[field]['previous_dtype']:
# complex128 will get casted to complex64 later
features[field]['previous_dtype'] = 'complex64'
# get fnc to use while reading
fnc = FEAT_FNC_SWITCH[batch[field].dtype.name]
# get dtype
if 'float' in fnc.__name__:
dtype = 'tf.float32'
elif 'complex' in fnc.__name__:
dtype = 'tf.float32'
elif 'int' in fnc.__name__:
dtype = 'tf.int64'
elif 'bytes' in fnc.__name__:
dtype = 'tf.string'
else:
raise ValueError('Cant find datatype')
features[field]['dtype'] = dtype
return features
def batch_to_nparray(batch):
np_batch = { k: np.asarray(v) for k,v in batch.items() }
return np_batch
def _assert_batch_has_expected_shape(batch, expected_shape, unbatch):
assert isinstance(batch, dict)
b_keys = batch.keys()
e_keys = expected_shape.keys()
if set(b_keys) != set(e_keys):
raise ValueError("Batch keys do not match json dictionary")
for key in b_keys:
if type(batch[key]) is not np.ndarray and type(batch[key]) is not np.array:
raise TypeError(
"Batch is a singular item, not np array ... maybe you meant to set unbatch=False?")
if batch[key].dtype.name != expected_shape[key]['previous_dtype']:
raise ValueError(
"Batch feature datatypes do not match json dictionary")
# exclude batch dim
if list(batch[key].shape[1:]) != list(expected_shape[key]['shape']):
raise ValueError(
"Batch feature shapes do not match json dictionary")
return True
def _assert_item_has_expected_shape(item, expected_shape, unbatch):
assert isinstance(item, dict)
r_keys = item.keys()
e_keys = expected_shape.keys()
if set(r_keys) != set(e_keys):
raise ValueError("Item key does not batch json dictionary")
for key in r_keys:
if item[key].dtype.name != expected_shape[key]['previous_dtype']:
raise ValueError(
"Item feature datatype does not batch json dictionary")
item_shape = item[key].shape if unbatch else item[key].shape[1:]
if list(item_shape) != list(expected_shape[key]['shape']):
print("key:", key)
print("item_shape:", list(item_shape))
print("expected_shape:", expected_shape[key]['shape'])
raise ValueError(
"Item shape does not match json dictionary")
return True
src_dir
from python.tfrecords.tfrecord_utils import replay_generator
from python.data_tools import crop_resize
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
filepath = './data/data-tfrecord-norm/' # relative to project_dir
ds_raw = replay_generator(filepath, parallel_files=1)
filepath
src_dir = filepath
parallel_files = 1
src_dir =os.path.abspath(src_dir)
json_path = os.path.join(src_dir, JSON_FNAME)
assert os.path.isfile(json_path)
filepath = '/home/jason/internal/bengali/data/data-tfrecord-norm/' # relative to project_dir
ds_raw = replay_generator(filepath, parallel_files=1)
ds_raw
library(magrittr)
library(purrr)
args = commandArgs(trailingOnly = TRUE)
print(args)
run_dir = "/home/jason/internal/runs/2020-02-17T12-50-36.370Z" # args[[1]]
if (length(args) > 1) {
flags <- tail(args, -1)
flags %<>% strsplit('=')
names(flags) <- map(flags, ~.x[[2]])
flags %<>% map(~.x[[2]])
} else
flags <- NULL
cat("Flags: \n")
print(flags)
cat("Run dir: \n")
print(run_dir)
tfruns::training_run(echo = FALSE, run_dir = run_dir, flags = flags)
getwd()
source("flags.R")
parser <- Xmisc::ArgumentParser$new()
parser$add_argument(
'--projdir', type = 'character',
default = '/home/jason/internal/bengali',
help = 'project directory to launch the training run from'
)
parser$add_argument(
'--rundir', type = 'character',
default = '/home/jason/internal/runs',
help = 'run directory to save results'
)
args = parser$get_args()
setwd(args$projdir)
options(tfruns.runs_dir = args$rundir)
rundir = tfruns::unique_run_dir(seconds_scale = 3)
system(paste0(
'rsync -az --exclude=runs, --exclude=logs',
' --exclude=.Rproj.user, --exclude=.git',
' --exclude=plots --exclude=logs --exclude=data',
' --exclude=lit ./ ', rundir))
# manually copy model file to model.R in toplevel of rsync'd dir for `restore_model()`
system(paste0('cp ', FLAGS$model, ' ', rundir, '/model.R'))
system(paste0('echo run_dir: ', rundir))
setwd(rundir)
system(paste0('\n tfrun ', rundir))
source("flags.R")
parser <- Xmisc::ArgumentParser$new()
parser$add_argument(
'--projdir', type = 'character',
default = '/home/jason/internal/bengali',
help = 'project directory to launch the training run from'
)
parser$add_argument(
'--rundir', type = 'character',
default = '/home/jason/internal/runs',
help = 'run directory to save results'
)
args = parser$get_args()
setwd(args$projdir)
options(tfruns.runs_dir = args$rundir)
rundir = tfruns::unique_run_dir(seconds_scale = 3)
system(paste0(
'rsync -az --exclude=runs, --exclude=logs',
' --exclude=.Rproj.user, --exclude=.git',
' --exclude=plots --exclude=logs --exclude=data',
' --exclude=lit ./ ', rundir))
# manually copy model file to model.R in toplevel of rsync'd dir for `restore_model()`
system(paste0('cp ', FLAGS$model, ' ', rundir, '/model.R'))
system(paste0('echo run_dir: ', rundir))
setwd(rundir)
system(paste0('\n tfrun ', rundir))
source('~/internal/bengali/exec/tfrun-local.R', echo=TRUE)
source('~/internal/bengali/exec/tfrun-local.R', echo=TRUE)
source('~/internal/bengali/exec/tfrun-local.R', echo=TRUE)
getwd()
setwd("~internal/bengali")
getwd()
source('~/internal/bengali/exec/tfrun-local.R', echo=TRUE)
source('~/internal/bengali/exec/tfrun-local.R', echo=TRUE)
getwd()
setwd("~/internal/bengali")
getwd()
reticulate::repl_python()
reticulate::source_python('~/internal/bengali/python/augmix2.py')
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
df
reticulate::repl_python()
reticulate::repl_python()
getwd()
getwd()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
