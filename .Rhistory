x$consonant <- tf$one_hot(x$consonant, length(CON$index)) %>% tf$squeeze(0L)
tuple(tf$expand_dims(tf$squeeze(x$image, 0L), -1L),
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
x
tuple(tf$expand_dims(tf$squeeze(x$image, 0L), -1L),
tuple(x$grapheme, x$consonant, x$vowel))
ds <- ds_raw %>%
dataset_map(function(x) {
browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index)) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index)) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index)) %>% tf$squeeze(0L)
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
ds <- ds_raw %>%
dataset_map(function(x) {
# browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index)) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index)) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index)) %>% tf$squeeze(0L)
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
ds
ds <- ds_raw %>%
dataset_map(function(x) {
# browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index), dtype = tf$int32) %>% tf$squeeze(0L)
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
ds
ds <- ds_raw %>%
dataset_map(function(x) {
# browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index), dtype = tf$int32) %>% tf$squeeze(0L)
a
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
ds <- ds_raw %>%
dataset_map(function(x) {
# browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index), dtype = tf$int32) %>% tf$squeeze(0L)
a
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
ds <- ds_raw %>%
dataset_map(function(x) {
# browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index), dtype = tf$int32) %>% tf$squeeze(0L)
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
ds <- ds_raw %>%
dataset_map(function(x) {
# browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index), dtype = tf$int32) %>% tf$squeeze(0L)
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
ds
(
val_ds <- ds$take(FLAGS$val_size)
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
import_from("dataset.R", ds, val_ds)
sa
ds
val_ds
# source("dataset-npz.R")
import_from("models/base.R", model)
callbacks <- list()
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = 10,
epochs = 100,
steps_per_epoch = 25,
callbacks = callbacks
)
labels
GPH
GPH %>% length
GPH$index %>% length
source_python("python/tfrecords/load_tfrecords.py")
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
ds <- ds_raw %>%
dataset_map(function(x) {
# browser()
x$image     <- tf$expand_dims(x$image, -1L) %>% tf$squeeze(0L)
x$grapheme  <- tf$one_hot(x$grapheme, length(GPH$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$vowel     <- tf$one_hot(x$vowel, length(VOW$index), dtype = tf$int32) %>% tf$squeeze(0L)
x$consonant <- tf$one_hot(x$consonant, length(CON$index), dtype = tf$int32) %>% tf$squeeze(0L)
tuple(x$image,
tuple(x$grapheme, x$consonant, x$vowel))
}) %>%
dataset_batch(FLAGS$batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(10L) %>%
dataset_prefetch(2L)
val_ds <- ds$take(FLAGS$val_size)
ds
length(GPH$index)
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
# source("dataset-npz.R")
import_from("dataset.R", ds, val_ds)
import_from("models/base.R", model)
callbacks <- list()
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = 10,
epochs = 100,
steps_per_epoch = 200,
callbacks = callbacks
)
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
# source("dataset-npz.R")
import_from("dataset.R", ds, val_ds)
import_from("models/base.R", model)
callbacks <- list()
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = FLAGS$val_size,
epochs = FLAGS$epochs,
steps_per_epoch = FLAGS$steps_per_epoch,
callbacks = callbacks
)
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = FLAGS$val_size,
epochs = FLAGS$epochs,
steps_per_epoch = 10,#FLAGS$steps_per_epoch,
callbacks = callbacks
)
callbacks <- #list()
list(
callback_reduce_lr_on_plateau(monitor = "loss"),
callback_model_checkpoint("model-weights-best-checkpoint.h5", monitor = "loss")
callback_tensorboard(file.path("logs", stringr::str_squish(lubridate::now())))
)
callbacks <- #list()
list(
callback_reduce_lr_on_plateau(monitor = "loss"),
callback_model_checkpoint("model-weights-best-checkpoint.h5", monitor = "loss"),
callback_tensorboard(file.path("logs", stringr::str_squish(lubridate::now())))
)
callbacks
model
model$loss
import_from("flags.R", FLAGS)
# source("dataset-npz.R")
import_from("dataset.R", ds, val_ds)
import_from("models/base.R", model)
model
callbacks <- #list()
list(
callback_reduce_lr_on_plateau(monitor = "grapheme_root_loss"),
callback_model_checkpoint("model-weights-best-checkpoint.h5", monitor = "acc"),
callback_tensorboard(file.path("logs", stringr::str_squish(lubridate::now())))
)
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = FLAGS$val_size,
epochs = FLAGS$epochs,
steps_per_epoch = FLAGS$steps_per_epoch,
callbacks = callbacks
)
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
# source("dataset-npz.R")
import_from("dataset.R", ds, val_ds)
import_from("models/base.R", model)
callbacks <- #list()
list(
callback_reduce_lr_on_plateau(monitor = "grapheme_root_loss"),
callback_reduce_lr_on_plateau(monitor = "consonant_loss"),
callback_reduce_lr_on_plateau(monitor = "vowel_loss"),
callback_model_checkpoint("model-weights-best-checkpoint.h5", monitor = "grapheme_root_acc"),
callback_tensorboard(file.path("logs", stringr::str_squish(lubridate::now())))
)
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = FLAGS$val_size,
epochs = FLAGS$epochs,
steps_per_epoch = FLAGS$steps_per_epoch,
callbacks = callbacks
)
reticulate::repl_python()
reticulate::repl_python()
library(keras)
input <- layer_input(shape = list(FLAGS$height, FLAGS$width, 1L))
import_from("flags.R", FLAGS)
input <- layer_input(shape = list(FLAGS$height, FLAGS$width, 1L))
base <- input %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_conv_2d(32L, 5L, activation = 'relu', padding = 'same') %>%
layer_dropout(rate = 0.3)
base
block1 <- base %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_global_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
block1 <- base %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_global_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
layer_global_average_pooling_1d
block1 <- base %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
features <- block1 %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_global_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
features <- block1 %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
features''
features
features_flat <- layer_flatten(features)
features_flat
dense <- features_flat %>%
layer_dense(1024, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(512, activation = 'relu')
dense
root <- features_dense %>%
layer_dense(256L) %>%
layer_dense(length(GPH$index), activation = 'softmax', name = "grapheme_root")
features_dense <- features_flat %>%
layer_dense(1024, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(512, activation = 'relu')
root <- features_dense %>%
layer_dense(256L) %>%
layer_dense(length(GPH$index), activation = 'softmax', name = "grapheme_root")
cons <- features_dense %>%
layer_dense(128L) %>%
layer_dense(length(CON$index), activation = 'softmax', name = "consonant")
vowel <- features_dense %>%
layer_dense(128L) %>%
layer_dense(length(VOW$index), activation = 'softmax', name = "vowel")
root
cons
vowel
features_dense
model <- keras_model(input, list(root, cons, vowel))
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = 'acc'
)
cat("Finished sourcing model with %s params\n", model$count_params())
model
features_flat <-
if (global_pool)
layer_flatten(features)
else
features_flat <-
if (global_pool) layer_flatten(features) else layer_global_max_pooling_2d(features)
global_pool  <- FALSE
features_flat <-
if (global_pool) layer_flatten(features) else layer_global_max_pooling_2d(features)
features_flat
features
layer_global_max_pooling_1d(features)
features <- block1 %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(256L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(512L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
features
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
input <- layer_input(shape = list(FLAGS$height, FLAGS$width, 1L))
base <- input %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(32L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_conv_2d(32L, 5L, activation = 'relu', padding = 'same') %>%
layer_dropout(rate = 0.3)
block1 <- base %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(64L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
features <- block1 %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(128L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(256L, 3L, activation = 'relu', padding = 'same') %>%
layer_conv_2d(512L, 3L, activation = 'relu', padding = 'same') %>%
layer_batch_normalization(momentum = 0.9) %>%
layer_max_pooling_2d(pool_size = list(2L, 2L)) %>%
layer_dropout(rate = 0.3)
features_flat <-
if (!FLAGSW$global_pool) layer_flatten(features) else layer_global_max_pooling_1d(features)
features_flat <-
if (!FLAGS$global_pool) layer_flatten(features) else layer_global_max_pooling_1d(features)
features_flat <-
if (!FLAGS$global_pool) layer_flatten(features) else layer_global_max_pooling_2d(features)
features_flat
features_dense <- features_flat %>%
layer_dense(512, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(256, activation = 'relu')
root <- features_dense %>%
layer_dense(192) %>%
layer_dense(length(GPH$index), activation = 'softmax', name = "grapheme_root")
cons <- features_dense %>%
layer_dense(64L) %>%
layer_dense(length(CON$index), activation = 'softmax', name = "consonant")
vowel <- features_dense %>%
layer_dense(64L) %>%
layer_dense(length(VOW$index), activation = 'softmax', name = "vowel")
model <- keras_model(input, list(root, cons, vowel))
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = 'acc'
)
model
model
source('~/internal/bengali/models/conv-simple.R')
model
source('~/internal/bengali/models/conv-simple.R')
model
# source("dataset-npz.R")
import_from("dataset.R", ds, val_ds)
callbacks <- #list()
list(
callback_reduce_lr_on_plateau(monitor = "grapheme_root_loss"),
callback_reduce_lr_on_plateau(monitor = "consonant_loss"),
callback_reduce_lr_on_plateau(monitor = "vowel_loss"),
callback_early_stopping(monitor = "val_grapheme_root_loss", patience = 25),
callback_model_checkpoint("model-weights-best-checkpoint.h5", monitor = "grapheme_root_acc"),
callback_tensorboard(file.path("logs", stringr::str_squish(lubridate::now())))
)
library(tfruns)
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = FLAGS$val_size,
epochs = FLAGS$epochs,
steps_per_epoch = FLAGS$steps_per_epoch,
callbacks = callbacks
)
# TODO: Add find model functionality from JSGutils/deepR
# import_from("models/base.R", model)
import_from("models/conv-simple.R", model)
import_from("flags.R", FLAGS)
# source("dataset-npz.R")
import_from("dataset.R", ds, val_ds)
ds
callbacks <- #list()
list(
callback_reduce_lr_on_plateau(monitor = "grapheme_root_loss"),
callback_reduce_lr_on_plateau(monitor = "consonant_loss"),
callback_reduce_lr_on_plateau(monitor = "vowel_loss"),
callback_early_stopping(monitor = "val_grapheme_root_loss", patience = 25),
callback_model_checkpoint("model-weights-best-checkpoint.h5", monitor = "grapheme_root_acc"),
callback_tensorboard(file.path("logs", stringr::str_squish(lubridate::now())))
)
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = FLAGS$val_size,
epochs = FLAGS$epochs,
steps_per_epoch = FLAGS$steps_per_epoch,
callbacks = callbacks
)
if (!exists_here("FLAGS")) {
import_from("flags.R", FLAGS)
}
# source("dataset-npz.R")
import_from("dataset.R", ds, val_ds)
# TODO: Add find model functionality from JSGutils/deepR
# import_from("models/base.R", model)
import_from("models/conv-simple.R", model)
callbacks <- #list()
list(
callback_reduce_lr_on_plateau(monitor = "grapheme_root_loss"),
callback_reduce_lr_on_plateau(monitor = "consonant_loss"),
callback_reduce_lr_on_plateau(monitor = "vowel_loss"),
callback_early_stopping(monitor = "val_grapheme_root_loss", patience = 25),
callback_model_checkpoint("model-weights-best-checkpoint.h5", monitor = "grapheme_root_acc"),
callback_tensorboard(file.path("logs", stringr::str_squish(lubridate::now())))
)
hist <- model %>%
fit(
ds,
validation_data = val_ds,
validation_steps = FLAGS$val_size,
epochs = FLAGS$epochs,
steps_per_epoch = FLAGS$steps_per_epoch,
callbacks = callbacks
)
plot(hist)
qs::qsave(hist, "history.qs")
# TODO: investigate images after reading TFRECORDS -> MAKE SURE THEY ARE LEGIT
# TODO: Try CLR in different modes.  [Triangular, Exponential]
# TODO: Add tfruns functionality to launch from terminal**
# TODO:
